
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Aviary - A tool to deploy and query LLMs.">
      
      
      
        <link rel="canonical" href="https://anyscale.github.io/aviary/models/">
      
      
      
      
      <link rel="icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.7">
    
    
      
        <title>RayLLM model registry - Aviary</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#rayllm-model-registry" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Aviary" class="md-header__button md-logo" aria-label="Aviary" data-md-component="logo">
      
  <img src="../img/ray.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Aviary
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              RayLLM model registry
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/anyscale/ray-llm" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
  </div>
  <div class="md-source__repository">
    anyscale/ray-llm
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Aviary" class="md-nav__button md-logo" aria-label="Aviary" data-md-component="logo">
      
  <img src="../img/ray.png" alt="logo">

    </a>
    Aviary
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/anyscale/ray-llm" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
  </div>
  <div class="md-source__repository">
    anyscale/ray-llm
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Aviary Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cli.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CLI
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../kuberay/deploy-on-gke/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deploying on GKE
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../kuberay/deploy-on-eks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deploying on EKS
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#modify-an-existing-model" class="md-nav__link">
    <span class="md-ellipsis">
      Modify an existing model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deployment-config" class="md-nav__link">
    <span class="md-ellipsis">
      Deployment config
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deployment config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#engine-config" class="md-nav__link">
    <span class="md-ellipsis">
      Engine config
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Engine config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vllm-engine-config" class="md-nav__link">
    <span class="md-ellipsis">
      vLLM Engine Config
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trtllm-engine-config" class="md-nav__link">
    <span class="md-ellipsis">
      TRTLLM Engine Config
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-engine-config" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Engine Config
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prepare-tensorrt-llm-models" class="md-nav__link">
    <span class="md-ellipsis">
      Prepare TensorRT-LLM models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-format" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt Format
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Prompt Format">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#schema" class="md-nav__link">
    <span class="md-ellipsis">
      Schema
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaling-config" class="md-nav__link">
    <span class="md-ellipsis">
      Scaling config
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adding-a-new-model" class="md-nav__link">
    <span class="md-ellipsis">
      Adding a new model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Adding a new model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adding-a-private-model" class="md-nav__link">
    <span class="md-ellipsis">
      Adding a private model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="rayllm-model-registry">RayLLM model registry</h1>
<p>Each model is defined by a YAML configuration file in this directory.</p>
<h2 id="modify-an-existing-model">Modify an existing model</h2>
<p>To modify an existing model, simply edit the YAML file for that model.
Each config file consists of three sections: </p>
<ul>
<li><code>deployment_config</code>, </li>
<li><code>engine_config</code>, </li>
<li><code>scaling_config</code>.</li>
</ul>
<p>It's best to check out examples of existing models to see how they are configured.</p>
<h2 id="deployment-config">Deployment config</h2>
<p>The <code>deployment_config</code> section corresponds to
<a href="https://docs.ray.io/en/latest/serve/production-guide/config.html">Ray Serve configuration</a>
and specifies how to <a href="https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html">auto-scale the model</a>
(via <code>autoscaling_config</code>) and what specific options you may need for your
Ray Actors during deployments (using <code>ray_actor_options</code>). We recommend using the values from our sample configuration files for <code>metrics_interval_s</code>, <code>look_back_period_s</code>, <code>smoothing_factor</code>, <code>downscale_delay_s</code> and <code>upscale_delay_s</code>. These are the configuration options you may want to modify:</p>
<ul>
<li><code>min_replicas</code>, <code>initial_replicas</code>, <code>max_replicas</code> - Minimum, initial and maximum number of replicas of the model to deploy on your Ray cluster.</li>
<li><code>max_concurrent_queries</code> - Maximum number of queries that a Ray Serve replica can process at a time. Additional queries are queued at the proxy.</li>
<li><code>target_num_ongoing_requests_per_replica</code> - Guides the auto-scaling behavior. If the average number of ongoing requests across replicas is above this number, Ray Serve attempts to scale up the number of replicas, and vice-versa for downscaling. We typically set this to ~40% of the <code>max_concurrent_queries</code>.</li>
<li><code>ray_actor_options</code> - Similar to the <code>resources_per_worker</code> configuration in the <code>scaling_config</code>. Refer to the <code>scaling_config</code> section for more guidance.</li>
</ul>
<h3 id="engine-config">Engine config</h3>
<p>Engine is the abstraction for interacting with a model. It is responsible for scheduling and running the model inside a Ray Actor worker group.</p>
<p>The <code>engine_config</code> section specifies the model ID (<code>model_id</code>), how to initialize it, and what parameters to use when generating tokens with an LLM.</p>
<p>RayLLM supports continuous batching, meaning incoming requests are processed as soon as they arrive, and can be added to batches that are already being processed. This means that the model is not slowed down by certain sentences taking longer to generate than others. RayLLM also supports quantization, meaning compressed models can be deployed with cheaper hardware requirements. For more details on using quantized models in RayLLM, see the <a href="continuous_batching/quantization/README.md">quantization guide</a>.</p>
<h4 id="vllm-engine-config">vLLM Engine Config</h4>
<ul>
<li><code>model_id</code> is the ID that refers to the model in the RayLLM or OpenAI API.</li>
<li><code>type</code> is the type of  inference engine. <code>VLLMEngine</code>, <code>TRTLLMEngine</code>, and <code>EmbeddingEngine</code> are currently supported.</li>
<li><code>engine_kwargs</code> and <code>max_total_tokens</code> are configuration options for the inference engine (e.g. gpu_memory_utilization, quantization, max_num_seqs and so on, see <a href="https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py#L11">more options</a>). These options may vary depending on the hardware accelerator type and model size. We have tuned the parameters in the configuration files included in RayLLM for you to use as reference.</li>
<li><code>generation</code> contains configurations related to default generation parameters such as <code>prompt_format</code> and <code>stopping_sequences</code>. More info about prompt format can be found <a href="#prompt-format">here</a>.</li>
<li><code>hf_model_id</code> is the Hugging Face model ID. This can also be a path to a local directory. If not specified, defaults to <code>model_id</code>.</li>
<li><code>runtime_env</code> is a dictionary that contains Ray runtime environment configuration. It allows you to set per-model pip packages and environment variables. See <a href="https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments">Ray documentation on Runtime Environments</a> for more information.</li>
<li><code>s3_mirror_config</code> is a dictionary that contains configuration for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads.</li>
<li><code>gcs_mirror_config</code> is a dictionary that contains configuration for loading the model from Google Cloud Storage instead of Hugging Face Hub. You can use this to speed up downloads.</li>
</ul>
<h4 id="trtllm-engine-config">TRTLLM Engine Config</h4>
<ul>
<li><code>model_id</code> is the ID that refers to the model in the RayLLM or OpenAI API.</li>
<li><code>type</code> is the type of  inference engine. <code>VLLMEngine</code>, <code>TRTLLMEngine</code>, and <code>EmbeddingEngine</code> are currently supported.</li>
<li><code>model_local_path</code> is the path to the TensorRT-LLM model directory.</li>
<li><code>s3_mirror_config</code> is a dictionary that contains configurations for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads.</li>
<li><code>generation</code> contains configurations related to default generation parameters such as <code>prompt_format</code> and <code>stopping_sequences</code>. More info about prompt format can be found <a href="#prompt-format">here</a>.</li>
<li><code>scheduler_policy</code> sets the scheduler policy to either <code>MAX_UTILIZATION</code> or <code>GUARANTEED_NO_EVICT</code>.
(<code>MAX_UTILIZATION</code> packs as many requests as the underlying TRT engine can support in any iteration of the InflightBatching generation loop. While this is expected to maximize GPU throughput, it might require that some requests be paused and restarted depending on peak KV cache memory availability.
<code>GUARANTEED_NO_EVICT</code> uses KV cache more conservatively and guarantees that a request, once started, runs to completion without eviction.)</li>
<li><code>logger_level</code> is to configure log level for TensorRT-LLM engine. ("VERBOSE", "INFO", "WARNING", "ERROR")</li>
<li><code>max_num_sequences</code> is the maximum number of requests/sequences that the backend can maintain state for.</li>
<li><code>max_tokens_in_paged_kv_cache</code> sets the maximum number of tokens in the paged kv cache.</li>
<li><code>kv_cache_free_gpu_mem_fraction</code> sets the K-V Cache free gpu memory fraction.</li>
</ul>
<h4 id="embedding-engine-config">Embedding Engine Config</h4>
<ul>
<li><code>model_id</code> is the ID that refers to the model in the RayLLM or OpenAI API.</li>
<li><code>type</code> is the type of inference engine. <code>VLLMEngine</code>, <code>TRTLLMEngine</code> and <code>EmbeddingEngine</code> are currently supported.</li>
<li><code>hf_model_id</code> is the Hugging Face model ID. This can also be a path to a local directory. If not specified, defaults to <code>model_id</code>.</li>
<li><code>max_total_tokens</code> is to configure number of the maximum length of each query.</li>
<li><code>max_batch_size</code> is to set the maximum batch size when queries are batched in the backend.</li>
</ul>
<h4 id="prepare-tensorrt-llm-models">Prepare TensorRT-LLM models</h4>
<p>You can follow the TensorRT-LLM example to generate the model.(https://github.com/NVIDIA/TensorRT-LLM/tree/v0.6.1/examples/llama). After generating the model, you can upload the model artifact to S3 and use the <code>s3_mirror_config</code> to load the model from S3. You can also place the model artifacts in a local directory and use the <code>model_local_path</code> to load the model from the local directory. See the <a href="continuous_batching/trtllm-meta-llama--Llama-2-7b-chat-hf.yaml">llama example</a> for more details.</p>
<h4 id="prompt-format">Prompt Format</h4>
<p>A prompt format is used to convert a chat completions API input into a prompt to feed into the LLM engine. The format is a dictionary where the key refers to one of the chat actors and the value is a string template for which to convert the content of the message into a string. Each message in the API input is formated into a string and these strings are assembled together to form the final prompt.</p>
<p>The string template should include the <code>{instruction}</code> keyword, which will be replaced with message content from the ChatCompletions API.</p>
<p>For example, if a user sends the following message for llama2-7b-chat-hf (<a href="continuous_batching/meta-llama--Llama-2-7b-chat-hf.yaml#L27-L33">prompt format</a>):</p>
<pre><code class="language-json">{
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;You are a helpful assistant.&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;What is the capital of France?&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;The capital of France is Paris.&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;What about Germany?&quot;
    }
  ]
}
</code></pre>
<p>The generated prompt that is sent to the LLM engine will be:</p>
<pre><code>[INST] &lt;&lt;SYS&gt;&gt;
You are a helpful assistant.
&lt;&lt;/SYS&gt;&gt;

What is the capital of France? [/INST] The capital of France is Paris. &lt;/s&gt;&lt;s&gt;[INST] What about Germany? [/INST]
</code></pre>
<h5 id="schema">Schema</h5>
<p>The following keys are supported:
* <code>system</code> - The system message. This is a message inserted at the beginning of the prompt to provide instructions for the LLM.
* <code>assistant</code> - The assistant message. These messages are from the past turns of the assistant as defined in the list of messages provided in the ChatCompletions API.
* <code>trailing_assistant</code> - The new assistant message. This is the message that the assistant will send in the current turn as generated by the LLM.
* <code>user</code> - The user message. This is the messages of the user as defined in the list of messages provided in the ChatCompletions API.</p>
<p>In addition, there some configurations to control the prompt formatting behavior:
* <code>default_system_message</code> - The default system message. This system message is used by default if one is not provided in the ChatCompletions API.
* <code>system_in_user</code> - Whether the system prompt should be included in the user prompt. If true, the user field should include '{system}'.
* <code>add_system_tags_even_if_message_is_empty</code> - Whether to include the system tags even if the user message is empty.
* <code>strip_whitespace</code> - Whether to automatically strip whitespace from left and right of the content for the messages provided in the ChatCompletions API.</p>
<p>You can see config in the <a href="#adding-a-new-model">Adding a new model</a> section below.</p>
<h3 id="scaling-config">Scaling config</h3>
<p>Finally, the <code>scaling_config</code> section specifies what resources should be used to serve the model - this corresponds to Ray AIR <a href="https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html">ScalingConfig</a>. Note that the <code>scaling_config</code> applies to each model replica, and not the entire model deployment (in other words, each replica will have <code>num_workers</code> workers).</p>
<ul>
<li><code>num_workers</code> - Number of workers (i.e. Ray Actors) for each replica of the model. This controls the tensor parallelism for the model.</li>
<li><code>num_gpus_per_worker</code> - Number of GPUs to be allocated per worker. Typically, this should be 1. </li>
<li><code>num_cpus_per_worker</code> - Number of CPUs to be allocated per worker. </li>
<li><code>placement_strategy</code> - Ray supports different <a href="https://docs.ray.io/en/latest/ray-core/scheduling/placement-group.html#placement-strategy">placement strategies</a> for guiding the physical distribution of workers. To ensure all workers are on the same node, use "STRICT_PACK".</li>
<li><code>resources_per_worker</code> - we use <code>resources_per_worker</code> to set <a href="https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#id1">Ray custom resources</a> and place the models on specific node types. The node resources are set in node definitions. Here are some node setup examples while using <a href="https://github.com/ray-project/ray-llm/tree/master/docs/kuberay">KubeRay</a> or <a href="https://github.com/ray-project/ray-llm/blob/master/deploy/ray/rayllm-cluster.yaml#L35">Ray Clusters</a>. If you're deploying locally, please refer to this <a href="https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#specifying-node-resources">guide</a>. An example configuration of <code>resources_per_worker</code> involves setting <code>accelerator_type_a10</code>: 0.01 for a Llama-2-7b model to be deployed on an A10 GPU. Note the small fraction here (0.01). The <code>num_gpus_per_worker</code> configuration along with number of GPUs available on the node will help limit the actual number of workers that Ray schedules on the node. </li>
</ul>
<p>If you need to learn more about a specific configuration option, or need to add a new one, don't hesitate to reach out to the team.</p>
<h2 id="adding-a-new-model">Adding a new model</h2>
<p>To add an entirely new model to the zoo, you will need to create a new YAML file.
This file should follow the naming convention 
<code>&lt;organisation-name&gt;--&lt;model-name&gt;-&lt;model-parameters&gt;-&lt;extra-info&gt;.yaml</code>. We recommend using one of the existing models as a template (ideally, one that is the same architecture as the model you are adding).</p>
<pre><code class="language-yaml"># true by default - you can set it to false to ignore this model
# during loading
enabled: true
deployment_config:
  # This corresponds to Ray Serve settings, as generated with
  # `serve build`.
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 90.0
  ray_actor_options:
    # Resources assigned to each model deployment. The deployment will be
    # initialized first, and then start prediction workers which actually hold the model.
    resources:
      accelerator_type_cpu: 0.01
engine_config:
  # Model id - this is a RayLLM id
  model_id: mosaicml/mpt-7b-instruct
  # Id of the model on Hugging Face Hub. Can also be a disk path. Defaults to model_id if not specified.
  hf_model_id: mosaicml/mpt-7b-instruct
  # LLM engine keyword arguments passed when constructing the model.
  engine_kwargs:
    trust_remote_code: true
  # Optional Ray Runtime Environment configuration. See Ray documentation for more details.
  # Add dependent libraries, environment variables, etc.
  runtime_env:
    env_vars:
      YOUR_ENV_VAR: &quot;your_value&quot;
  # Optional configuration for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads or load models not on Hugging Face Hub.
  s3_mirror_config:
    bucket_uri: s3://large-dl-models-mirror/models--mosaicml--mpt-7b-instruct/main-safetensors/
  generation:
    # Format to convert user API input into prompts to feed into the LLM engine. {instruction} refers to user-supplied input.
    prompt_format:
      system: &quot;{instruction}\n&quot;  # System message. Will default to default_system_message
      assistant: &quot;### Response:\n{instruction}\n&quot;  # Past assistant message. Used in chat completions API.
      trailing_assistant: &quot;### Response:\n&quot;  # New assistant message. After this point, model will generate tokens.
      user: &quot;### Instruction:\n{instruction}\n&quot;  # User message.
      default_system_message: &quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.&quot;  # Default system message.
      system_in_user: false  # Whether the system prompt is inside the user prompt. If true, the user field should include '{system}'
      add_system_tags_even_if_message_is_empty: false  # Whether to include the system tags even if the user message is empty.
      strip_whitespace: false  # Whether to automaticall strip whitespace from left and right of user supplied messages for chat completions
    # Stopping sequences. The generation will stop when it encounters any of the sequences, or the tokenizer EOS token.
    # Those can be strings, integers (token ids) or lists of integers.
    # Stopping sequences supplied by the user in a request will be appended to this.
    stopping_sequences: [&quot;### Response:&quot;, &quot;### End&quot;]

# Resources assigned to each model replica. This corresponds to Ray AIR ScalingConfig.
scaling_config:
  # If using multiple GPUs set num_gpus_per_worker to be 1 and then set num_workers to be the number of GPUs you want to use.
  num_workers: 1
  num_gpus_per_worker: 1
  num_cpus_per_worker: 4
  resources_per_worker:
    # You can use custom resources to specify the instance type / accelerator type
    # to use for the model.
    accelerator_type_a10: 0.01

</code></pre>
<h3 id="adding-a-private-model">Adding a private model</h3>
<p>To add a private model, you can either choose to use a filesystem path or an S3/GCS mirror.</p>
<ul>
<li>For loading a model from file system, set <code>engine_config.hf_model_id</code> to an absolute filesystem path accessible from every node in the cluster and set <code>engine_config.model_id</code> to any ID you desire in the <code>organization/model</code> format, eg. <code>myorganization/llama2-finetuned</code>.</li>
<li>For loading a model from S3 or GCS, set <code>engine_config.s3_mirror_config.bucket_uri</code> or <code>engine_config.gcs_mirror_config.bucket_uri</code> to point to a folder containing your model and tokenizer files (<code>config.json</code>, <code>tokenizer_config.json</code>, <code>.bin</code>/<code>.safetensors</code> files, etc.) and set <code>engine_config.model_id</code> to any ID you desire in the <code>organization/model</code> format, eg. <code>myorganization/llama2-finetuned</code>. The model will be downloaded to a folder in the <code>&lt;TRANSFORMERS_CACHE&gt;/models--&lt;organization-name&gt;--&lt;model-name&gt;/snapshots/&lt;HASH&gt;</code> directory on each node in the cluster. <code>&lt;HASH&gt;</code> will be determined by the contents of <code>hash</code> file in the S3 folder, or default to <code>0000000000000000000000000000000000000000</code>. See the <a href="https://huggingface.co/docs/transformers/main/en/installation#cache-setup">HuggingFace transformers documentation</a>.</li>
</ul>
<p>For loading a model from your local file system:</p>
<pre><code class="language-yaml">engine_config:
  model_id: YOUR_MODEL_NAME
  hf_model_id: YOUR_MODEL_LOCAL_PATH
</code></pre>
<p>For loading a model from S3:</p>
<pre><code class="language-yaml">engine_config:
  model_id: YOUR_MODEL_NAME
  s3_mirror_config:
    bucket_uri: s3://YOUR_BUCKET_NAME/YOUR_MODEL_FOLDER

</code></pre>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/anyscale/aviary" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/raydistributed" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://anyscale.com" target="_blank" rel="noopener" title="anyscale.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M352 256c0 22.2-1.2 43.6-3.3 64H163.4c-2.2-20.4-3.3-41.8-3.3-64s1.2-43.6 3.3-64h185.3c2.2 20.4 3.3 41.8 3.3 64m28.8-64h123.1c5.3 20.5 8.1 41.9 8.1 64s-2.8 43.5-8.1 64H380.8c2.1-20.6 3.2-42 3.2-64s-1.1-43.4-3.2-64m112.6-32H376.7c-10-63.9-29.8-117.4-55.3-151.6 78.3 20.7 142 77.5 171.9 151.6zm-149.1 0H167.7c6.1-36.4 15.5-68.6 27-94.7 10.5-23.6 22.2-40.7 33.5-51.5C239.4 3.2 248.7 0 256 0s16.6 3.2 27.8 13.8c11.3 10.8 23 27.9 33.5 51.5 11.6 26 20.9 58.2 27 94.7m-209 0H18.6c30-74.1 93.6-130.9 172-151.6-25.5 34.2-45.3 87.7-55.3 151.6M8.1 192h123.1c-2.1 20.6-3.2 42-3.2 64s1.1 43.4 3.2 64H8.1C2.8 299.5 0 278.1 0 256s2.8-43.5 8.1-64m186.6 254.6c-11.6-26-20.9-58.2-27-94.6h176.6c-6.1 36.4-15.5 68.6-27 94.6-10.5 23.6-22.2 40.7-33.5 51.5-11.2 10.7-20.5 13.9-27.8 13.9s-16.6-3.2-27.8-13.8c-11.3-10.8-23-27.9-33.5-51.5zM135.3 352c10 63.9 29.8 117.4 55.3 151.6-78.4-20.7-142-77.5-172-151.6zm358.1 0c-30 74.1-93.6 130.9-171.9 151.6 25.5-34.2 45.2-87.7 55.3-151.6h116.7z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>