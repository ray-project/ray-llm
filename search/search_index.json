{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"DOCKERHUB/","title":"DOCKERHUB","text":""},{"location":"DOCKERHUB/#overview","title":"Overview","text":"<p>This is the publicly available set of Docker images for Anyscale/Ray's RayLLM (formerly Aviary) project.</p> <p>RayLLM is an LLM serving solution that makes it easy to deploy and manage a variety of open source LLMs. It does this by:</p> <ul> <li>Providing an extensive suite of pre-configured open source LLMs, with defaults that work out of the box.</li> <li>Supporting Transformer models hosted on Hugging Face Hub or present on local disk.</li> <li>Simplifying the deployment of multiple LLMs within a single unified framework.</li> <li>Simplifying the addition of new LLMs to within minutes in most cases.</li> <li>Offering unique autoscaling support, including scale-to-zero.</li> <li>Fully supporting multi-GPU &amp; multi-node model deployments.</li> <li>Offering high performance features like continuous batching, quantization and streaming.</li> <li>Providing a REST API that is similar to OpenAI's to make it easy to migrate and cross test them.</li> </ul> <p>Read more here</p>"},{"location":"DOCKERHUB/#tags","title":"Tags","text":"Name Notes <code>:0.3.1</code> Release v0.3.1 <code>:0.3.0</code> Release v0.3.0 <code>:latest</code> Most recently pushed version release image"},{"location":"DOCKERHUB/#usage","title":"Usage","text":"<p>See: ray-project/ray-llm \"Deploying RayLLM\" for full instructions</p>"},{"location":"DOCKERHUB/#example","title":"Example","text":"<p>Requires a machine with compatible NVIDIA A10 GPU and valid <code>HUGGING_FACE_HUB_TOKEN</code> to run the Amazon LightGPT model:</p> <pre><code>docker run \\\n    --gpus all \\\n    -e HUGGING_FACE_HUB_TOKEN=&lt;your_token&gt; \\\n    --shm-size 1g \\\n    -p 8000:8000 \\\n    --entrypoint rayllm \\\n    anyscale/ray-llm:latest run --model models/continuous_batching/amazon--LightGPT.yaml\n</code></pre>"},{"location":"DOCKERHUB/#source","title":"Source","text":"<p>Source is available at https://github.com/ray-project/ray-llm</p>"},{"location":"models/","title":"RayLLM model registry","text":"<p>Each model is defined by a YAML configuration file in this directory.</p>"},{"location":"models/#modify-an-existing-model","title":"Modify an existing model","text":"<p>To modify an existing model, simply edit the YAML file for that model. Each config file consists of three sections: </p> <ul> <li><code>deployment_config</code>, </li> <li><code>engine_config</code>, </li> <li><code>scaling_config</code>.</li> </ul> <p>It's best to check out examples of existing models to see how they are configured.</p>"},{"location":"models/#deployment-config","title":"Deployment config","text":"<p>The <code>deployment_config</code> section corresponds to Ray Serve configuration and specifies how to auto-scale the model (via <code>autoscaling_config</code>) and what specific options you may need for your Ray Actors during deployments (using <code>ray_actor_options</code>). We recommend using the values from our sample configuration files for <code>metrics_interval_s</code>, <code>look_back_period_s</code>, <code>smoothing_factor</code>, <code>downscale_delay_s</code> and <code>upscale_delay_s</code>. These are the configuration options you may want to modify:</p> <ul> <li><code>min_replicas</code>, <code>initial_replicas</code>, <code>max_replicas</code> - Minimum, initial and maximum number of replicas of the model to deploy on your Ray cluster.</li> <li><code>max_concurrent_queries</code> - Maximum number of queries that a Ray Serve replica can process at a time. Additional queries are queued at the proxy.</li> <li><code>target_num_ongoing_requests_per_replica</code> - Guides the auto-scaling behavior. If the average number of ongoing requests across replicas is above this number, Ray Serve attempts to scale up the number of replicas, and vice-versa for downscaling. We typically set this to ~40% of the <code>max_concurrent_queries</code>.</li> <li><code>ray_actor_options</code> - Similar to the <code>resources_per_worker</code> configuration in the <code>scaling_config</code>. Refer to the <code>scaling_config</code> section for more guidance.</li> </ul>"},{"location":"models/#engine-config","title":"Engine config","text":"<p>Engine is the abstraction for interacting with a model. It is responsible for scheduling and running the model inside a Ray Actor worker group.</p> <p>The <code>engine_config</code> section specifies the model ID (<code>model_id</code>), how to initialize it, and what parameters to use when generating tokens with an LLM.</p> <p>RayLLM supports continuous batching, meaning incoming requests are processed as soon as they arrive, and can be added to batches that are already being processed. This means that the model is not slowed down by certain sentences taking longer to generate than others. RayLLM also supports quantization, meaning compressed models can be deployed with cheaper hardware requirements. For more details on using quantized models in RayLLM, see the quantization guide.</p>"},{"location":"models/#vllm-engine-config","title":"vLLM Engine Config","text":"<ul> <li><code>model_id</code> is the ID that refers to the model in the RayLLM or OpenAI API.</li> <li><code>type</code> is the type of  inference engine. <code>VLLMEngine</code>, <code>TRTLLMEngine</code>, and <code>EmbeddingEngine</code> are currently supported.</li> <li><code>engine_kwargs</code> and <code>max_total_tokens</code> are configuration options for the inference engine (e.g. gpu_memory_utilization, quantization, max_num_seqs and so on, see more options). These options may vary depending on the hardware accelerator type and model size. We have tuned the parameters in the configuration files included in RayLLM for you to use as reference.</li> <li><code>generation</code> contains configurations related to default generation parameters such as <code>prompt_format</code> and <code>stopping_sequences</code>. More info about prompt format can be found here.</li> <li><code>hf_model_id</code> is the Hugging Face model ID. This can also be a path to a local directory. If not specified, defaults to <code>model_id</code>.</li> <li><code>runtime_env</code> is a dictionary that contains Ray runtime environment configuration. It allows you to set per-model pip packages and environment variables. See Ray documentation on Runtime Environments for more information.</li> <li><code>s3_mirror_config</code> is a dictionary that contains configuration for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads.</li> <li><code>gcs_mirror_config</code> is a dictionary that contains configuration for loading the model from Google Cloud Storage instead of Hugging Face Hub. You can use this to speed up downloads.</li> </ul>"},{"location":"models/#trtllm-engine-config","title":"TRTLLM Engine Config","text":"<ul> <li><code>model_id</code> is the ID that refers to the model in the RayLLM or OpenAI API.</li> <li><code>type</code> is the type of  inference engine. <code>VLLMEngine</code>, <code>TRTLLMEngine</code>, and <code>EmbeddingEngine</code> are currently supported.</li> <li><code>model_local_path</code> is the path to the TensorRT-LLM model directory.</li> <li><code>s3_mirror_config</code> is a dictionary that contains configurations for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads.</li> <li><code>generation</code> contains configurations related to default generation parameters such as <code>prompt_format</code> and <code>stopping_sequences</code>. More info about prompt format can be found here.</li> <li><code>scheduler_policy</code> sets the scheduler policy to either <code>MAX_UTILIZATION</code> or <code>GUARANTEED_NO_EVICT</code>. (<code>MAX_UTILIZATION</code> packs as many requests as the underlying TRT engine can support in any iteration of the InflightBatching generation loop. While this is expected to maximize GPU throughput, it might require that some requests be paused and restarted depending on peak KV cache memory availability. <code>GUARANTEED_NO_EVICT</code> uses KV cache more conservatively and guarantees that a request, once started, runs to completion without eviction.)</li> <li><code>logger_level</code> is to configure log level for TensorRT-LLM engine. (\"VERBOSE\", \"INFO\", \"WARNING\", \"ERROR\")</li> <li><code>max_num_sequences</code> is the maximum number of requests/sequences that the backend can maintain state for.</li> <li><code>max_tokens_in_paged_kv_cache</code> sets the maximum number of tokens in the paged kv cache.</li> <li><code>kv_cache_free_gpu_mem_fraction</code> sets the K-V Cache free gpu memory fraction.</li> </ul>"},{"location":"models/#embedding-engine-config","title":"Embedding Engine Config","text":"<ul> <li><code>model_id</code> is the ID that refers to the model in the RayLLM or OpenAI API.</li> <li><code>type</code> is the type of inference engine. <code>VLLMEngine</code>, <code>TRTLLMEngine</code> and <code>EmbeddingEngine</code> are currently supported.</li> <li><code>hf_model_id</code> is the Hugging Face model ID. This can also be a path to a local directory. If not specified, defaults to <code>model_id</code>.</li> <li><code>max_total_tokens</code> is to configure number of the maximum length of each query.</li> <li><code>max_batch_size</code> is to set the maximum batch size when queries are batched in the backend.</li> </ul>"},{"location":"models/#prepare-tensorrt-llm-models","title":"Prepare TensorRT-LLM models","text":"<p>You can follow the TensorRT-LLM example to generate the model.(https://github.com/NVIDIA/TensorRT-LLM/tree/v0.6.1/examples/llama). After generating the model, you can upload the model artifact to S3 and use the <code>s3_mirror_config</code> to load the model from S3. You can also place the model artifacts in a local directory and use the <code>model_local_path</code> to load the model from the local directory. See the llama example for more details.</p>"},{"location":"models/#prompt-format","title":"Prompt Format","text":"<p>A prompt format is used to convert a chat completions API input into a prompt to feed into the LLM engine. The format is a dictionary where the key refers to one of the chat actors and the value is a string template for which to convert the content of the message into a string. Each message in the API input is formated into a string and these strings are assembled together to form the final prompt.</p> <p>The string template should include the <code>{instruction}</code> keyword, which will be replaced with message content from the ChatCompletions API.</p> <p>For example, if a user sends the following message for llama2-7b-chat-hf (prompt format):</p> <pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the capital of France?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"The capital of France is Paris.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What about Germany?\"\n    }\n  ]\n}\n</code></pre> <p>The generated prompt that is sent to the LLM engine will be:</p> <pre><code>[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nWhat is the capital of France? [/INST] The capital of France is Paris. &lt;/s&gt;&lt;s&gt;[INST] What about Germany? [/INST]\n</code></pre>"},{"location":"models/#schema","title":"Schema","text":"<p>The following keys are supported: * <code>system</code> - The system message. This is a message inserted at the beginning of the prompt to provide instructions for the LLM. * <code>assistant</code> - The assistant message. These messages are from the past turns of the assistant as defined in the list of messages provided in the ChatCompletions API. * <code>trailing_assistant</code> - The new assistant message. This is the message that the assistant will send in the current turn as generated by the LLM. * <code>user</code> - The user message. This is the messages of the user as defined in the list of messages provided in the ChatCompletions API.</p> <p>In addition, there some configurations to control the prompt formatting behavior: * <code>default_system_message</code> - The default system message. This system message is used by default if one is not provided in the ChatCompletions API. * <code>system_in_user</code> - Whether the system prompt should be included in the user prompt. If true, the user field should include '{system}'. * <code>add_system_tags_even_if_message_is_empty</code> - Whether to include the system tags even if the user message is empty. * <code>strip_whitespace</code> - Whether to automatically strip whitespace from left and right of the content for the messages provided in the ChatCompletions API.</p> <p>You can see config in the Adding a new model section below.</p>"},{"location":"models/#scaling-config","title":"Scaling config","text":"<p>Finally, the <code>scaling_config</code> section specifies what resources should be used to serve the model - this corresponds to Ray AIR ScalingConfig. Note that the <code>scaling_config</code> applies to each model replica, and not the entire model deployment (in other words, each replica will have <code>num_workers</code> workers).</p> <ul> <li><code>num_workers</code> - Number of workers (i.e. Ray Actors) for each replica of the model. This controls the tensor parallelism for the model.</li> <li><code>num_gpus_per_worker</code> - Number of GPUs to be allocated per worker. Typically, this should be 1. </li> <li><code>num_cpus_per_worker</code> - Number of CPUs to be allocated per worker. </li> <li><code>placement_strategy</code> - Ray supports different placement strategies for guiding the physical distribution of workers. To ensure all workers are on the same node, use \"STRICT_PACK\".</li> <li><code>resources_per_worker</code> - we use <code>resources_per_worker</code> to set Ray custom resources and place the models on specific node types. The node resources are set in node definitions. Here are some node setup examples while using KubeRay or Ray Clusters. If you're deploying locally, please refer to this guide. An example configuration of <code>resources_per_worker</code> involves setting <code>accelerator_type_a10</code>: 0.01 for a Llama-2-7b model to be deployed on an A10 GPU. Note the small fraction here (0.01). The <code>num_gpus_per_worker</code> configuration along with number of GPUs available on the node will help limit the actual number of workers that Ray schedules on the node. </li> </ul> <p>If you need to learn more about a specific configuration option, or need to add a new one, don't hesitate to reach out to the team.</p>"},{"location":"models/#adding-a-new-model","title":"Adding a new model","text":"<p>To add an entirely new model to the zoo, you will need to create a new YAML file. This file should follow the naming convention  <code>&lt;organisation-name&gt;--&lt;model-name&gt;-&lt;model-parameters&gt;-&lt;extra-info&gt;.yaml</code>. We recommend using one of the existing models as a template (ideally, one that is the same architecture as the model you are adding).</p> <pre><code># true by default - you can set it to false to ignore this model\n# during loading\nenabled: true\ndeployment_config:\n  # This corresponds to Ray Serve settings, as generated with\n  # `serve build`.\n  autoscaling_config:\n    min_replicas: 1\n    initial_replicas: 1\n    max_replicas: 8\n    target_num_ongoing_requests_per_replica: 1.0\n    metrics_interval_s: 10.0\n    look_back_period_s: 30.0\n    smoothing_factor: 1.0\n    downscale_delay_s: 300.0\n    upscale_delay_s: 90.0\n  ray_actor_options:\n    # Resources assigned to each model deployment. The deployment will be\n    # initialized first, and then start prediction workers which actually hold the model.\n    resources:\n      accelerator_type_cpu: 0.01\nengine_config:\n  # Model id - this is a RayLLM id\n  model_id: mosaicml/mpt-7b-instruct\n  # Id of the model on Hugging Face Hub. Can also be a disk path. Defaults to model_id if not specified.\n  hf_model_id: mosaicml/mpt-7b-instruct\n  # LLM engine keyword arguments passed when constructing the model.\n  engine_kwargs:\n    trust_remote_code: true\n  # Optional Ray Runtime Environment configuration. See Ray documentation for more details.\n  # Add dependent libraries, environment variables, etc.\n  runtime_env:\n    env_vars:\n      YOUR_ENV_VAR: \"your_value\"\n  # Optional configuration for loading the model from S3 instead of Hugging Face Hub. You can use this to speed up downloads or load models not on Hugging Face Hub.\n  s3_mirror_config:\n    bucket_uri: s3://large-dl-models-mirror/models--mosaicml--mpt-7b-instruct/main-safetensors/\n  generation:\n    # Format to convert user API input into prompts to feed into the LLM engine. {instruction} refers to user-supplied input.\n    prompt_format:\n      system: \"{instruction}\\n\"  # System message. Will default to default_system_message\n      assistant: \"### Response:\\n{instruction}\\n\"  # Past assistant message. Used in chat completions API.\n      trailing_assistant: \"### Response:\\n\"  # New assistant message. After this point, model will generate tokens.\n      user: \"### Instruction:\\n{instruction}\\n\"  # User message.\n      default_system_message: \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"  # Default system message.\n      system_in_user: false  # Whether the system prompt is inside the user prompt. If true, the user field should include '{system}'\n      add_system_tags_even_if_message_is_empty: false  # Whether to include the system tags even if the user message is empty.\n      strip_whitespace: false  # Whether to automaticall strip whitespace from left and right of user supplied messages for chat completions\n    # Stopping sequences. The generation will stop when it encounters any of the sequences, or the tokenizer EOS token.\n    # Those can be strings, integers (token ids) or lists of integers.\n    # Stopping sequences supplied by the user in a request will be appended to this.\n    stopping_sequences: [\"### Response:\", \"### End\"]\n\n# Resources assigned to each model replica. This corresponds to Ray AIR ScalingConfig.\nscaling_config:\n  # If using multiple GPUs set num_gpus_per_worker to be 1 and then set num_workers to be the number of GPUs you want to use.\n  num_workers: 1\n  num_gpus_per_worker: 1\n  num_cpus_per_worker: 4\n  resources_per_worker:\n    # You can use custom resources to specify the instance type / accelerator type\n    # to use for the model.\n    accelerator_type_a10: 0.01\n\n</code></pre>"},{"location":"models/#adding-a-private-model","title":"Adding a private model","text":"<p>To add a private model, you can either choose to use a filesystem path or an S3/GCS mirror.</p> <ul> <li>For loading a model from file system, set <code>engine_config.hf_model_id</code> to an absolute filesystem path accessible from every node in the cluster and set <code>engine_config.model_id</code> to any ID you desire in the <code>organization/model</code> format, eg. <code>myorganization/llama2-finetuned</code>.</li> <li>For loading a model from S3 or GCS, set <code>engine_config.s3_mirror_config.bucket_uri</code> or <code>engine_config.gcs_mirror_config.bucket_uri</code> to point to a folder containing your model and tokenizer files (<code>config.json</code>, <code>tokenizer_config.json</code>, <code>.bin</code>/<code>.safetensors</code> files, etc.) and set <code>engine_config.model_id</code> to any ID you desire in the <code>organization/model</code> format, eg. <code>myorganization/llama2-finetuned</code>. The model will be downloaded to a folder in the <code>&lt;TRANSFORMERS_CACHE&gt;/models--&lt;organization-name&gt;--&lt;model-name&gt;/snapshots/&lt;HASH&gt;</code> directory on each node in the cluster. <code>&lt;HASH&gt;</code> will be determined by the contents of <code>hash</code> file in the S3 folder, or default to <code>0000000000000000000000000000000000000000</code>. See the HuggingFace transformers documentation.</li> </ul> <p>For loading a model from your local file system:</p> <pre><code>engine_config:\n  model_id: YOUR_MODEL_NAME\n  hf_model_id: YOUR_MODEL_LOCAL_PATH\n</code></pre> <p>For loading a model from S3:</p> <pre><code>engine_config:\n  model_id: YOUR_MODEL_NAME\n  s3_mirror_config:\n    bucket_uri: s3://YOUR_BUCKET_NAME/YOUR_MODEL_FOLDER\n\n</code></pre>"},{"location":"kuberay/deploy-on-eks/","title":"Deploy RayLLM on Amazon EKS using KubeRay","text":"<ul> <li>Note that this document will be extended to include Ray autoscaling and the deployment of multiple models in the near future.</li> </ul>"},{"location":"kuberay/deploy-on-eks/#part-1-set-up-a-kubernetes-cluster-on-amazon-eks","title":"Part 1: Set up a Kubernetes cluster on Amazon EKS","text":""},{"location":"kuberay/deploy-on-eks/#step-1-create-a-kubernetes-cluster-on-amazon-eks","title":"Step 1: Create a Kubernetes cluster on Amazon EKS","text":"<p>Follow the first two steps in this AWS documentation to: (1) Create your Amazon EKS cluster (2) Configure your computer to communicate with your cluster.</p>"},{"location":"kuberay/deploy-on-eks/#step-2-create-node-groups-for-the-amazon-eks-cluster","title":"Step 2: Create node groups for the Amazon EKS cluster","text":"<p>You can follow \"Step 3: Create nodes\" in this AWS documentation to create node groups. The following section provides more detailed information.</p>"},{"location":"kuberay/deploy-on-eks/#create-a-cpu-node-group","title":"Create a CPU node group","text":"<p>Create a CPU node group for all Pods except Ray GPU workers, such as KubeRay operator, Ray head, and CoreDNS Pods.</p> <ul> <li>Create a CPU node group</li> <li>Instance type: m5.xlarge (4 vCPU; 16 GB RAM)</li> <li>Disk size: 256 GB</li> <li>Desired size: 1, Min size: 0, Max size: 1</li> </ul>"},{"location":"kuberay/deploy-on-eks/#create-a-gpu-node-group","title":"Create a GPU node group","text":"<p>Create a GPU node group for Ray GPU workers.</p> <ul> <li>Create a GPU node group</li> <li>Add a Kubernetes taint to prevent CPU Pods from being scheduled on this GPU node group<ul> <li>Key: ray.io/node-type, Value: worker, Effect: NoSchedule</li> </ul> </li> <li>AMI type: Bottlerocket NVIDIA (BOTTLEROCKET_x86_64_NVIDIA)</li> <li>Instance type: g5.12xlarge (4 GPU; 96 GB GPU Memory; 48 vCPUs; 192 GB RAM)</li> <li>Disk size: 1024 GB</li> <li>Desired size: 1, Min size: 0, Max size: 1</li> </ul> <p>Because this tutorial is for deploying 1 LLM, the maximum size of this GPU node group is 1. If you want to deploy multiple LLMs in this cluster, you may need to increase the value of the max size.</p> <p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p>"},{"location":"kuberay/deploy-on-eks/#step-3-verify-the-node-groups","title":"Step 3: Verify the node groups","text":"<p>If you encounter permission issues with <code>eksctl</code>, you can navigate to your AWS account's webpage and copy the credential environment variables, including <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>, from the \"Command line or programmatic access\" page.</p> <pre><code>eksctl get nodegroup --cluster ${YOUR_EKS_NAME}\n\n# CLUSTER         NODEGROUP       STATUS  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID                        ASG NAME                           TYPE\n# ${YOUR_EKS_NAME}     cpu-node-group  ACTIVE  2023-06-05T21:31:49Z    0               1               1                       m5.xlarge       AL2_x86_64                      eks-cpu-node-group-...     managed\n# ${YOUR_EKS_NAME}     gpu-node-group  ACTIVE  2023-06-05T22:01:44Z    0               1               1                       g5.12xlarge     BOTTLEROCKET_x86_64_NVIDIA      eks-gpu-node-group-...     managed\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#step-4-install-the-daemonset-for-nvidia-device-plugin-for-kubernetes","title":"Step 4: Install the DaemonSet for NVIDIA device plugin for Kubernetes","text":"<p>If you encounter permission issues with <code>kubectl</code>, you can follow \"Step 2: Configure your computer to communicate with your cluster\" in the AWS documentation.</p> <p>You can refer to the Amazon EKS optimized accelerated Amazon Linux AMIs or NVIDIA/k8s-device-plugin repository for more details.</p> <pre><code># Install the DaemonSet\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.9.0/nvidia-device-plugin.yml\n\n# Verify that your nodes have allocatable GPUs \nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ip-....us-west-2.compute.internal   4\n# ip-....us-west-2.compute.internal   &lt;none&gt;\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#part-2-install-a-kuberay-operator","title":"Part 2: Install a KubeRay operator","text":"<pre><code># Install both CRDs and KubeRay operator v0.6.0.\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.6.0\n\n# It should be scheduled on the CPU node. If it is not, something is wrong.\n</code></pre> <p>At this point, you have two options:</p> <ol> <li>You can deploy RayLLM manually on a <code>RayCluster</code> (Part 3), or</li> <li>You can deploy RayLLM using a <code>RayService</code> custom resource (Part 4).</li> </ol> <p>The first option is more flexible for conducting experiments. The second option is recommended for production use due to the additional high availability features provided by the <code>RayService</code> custom resource, which will manage the underlying <code>RayCluster</code>s for you.</p>"},{"location":"kuberay/deploy-on-eks/#part-3-deploy-rayllm-on-a-raycluster-recommended-for-experiments","title":"Part 3: Deploy RayLLM on a RayCluster (recommended for experiments)","text":""},{"location":"kuberay/deploy-on-eks/#step-1-create-a-raycluster-with-rayllm","title":"Step 1: Create a RayCluster with RayLLM","text":"<pre><code># path: docs/kuberay\nkubectl apply -f ray-cluster.rayllm-eks.yaml\n</code></pre> <p>Something is worth noticing:</p> <ul> <li> <p>The <code>tolerations</code> for workers must match the taints on the GPU node group.</p> <p>```yaml</p> </li> <li> <p>Update <code>rayStartParams.resources</code> for Ray scheduling. The <code>OpenAssistant--falcon-7b-sft-top1-696.yaml</code> file uses both <code>accelerator_type_cpu</code> and <code>accelerator_type_a10</code>.</p> <p>```yaml</p> </li> </ul>"},{"location":"kuberay/deploy-on-eks/#please-add-the-following-taints-to-the-gpu-node","title":"Please add the following taints to the GPU node.","text":"<p>tolerations:     - key: \"ray.io/node-type\"     operator: \"Equal\"     value: \"worker\"     effect: \"NoSchedule\" ```</p>"},{"location":"kuberay/deploy-on-eks/#ray-head-the-ray-head-has-a-pod-resource-limit-of-2-cpus","title":"Ray head: The Ray head has a Pod resource limit of 2 CPUs.","text":"<p>rayStartParams:   resources: '\"{\\\"accelerator_type_cpu\\\": 2}\"'</p>"},{"location":"kuberay/deploy-on-eks/#ray-workers-the-ray-worker-has-a-pod-resource-limit-of-48-cpus-and-4-gpus","title":"Ray workers: The Ray worker has a Pod resource limit of 48 CPUs and 4 GPUs.","text":""},{"location":"kuberay/deploy-on-eks/#accelerator_type_a10-and-accelerator_type_a100_80g-below-are-only-used-for-ray-logical-resource-scheduling","title":"<code>accelerator_type_a10</code> and <code>accelerator_type_a100_80g</code> below are only used for Ray logical-resource scheduling.","text":""},{"location":"kuberay/deploy-on-eks/#this-does-not-imply-that-each-worker-has-2-a10-gpus-and-2-a100-gpus","title":"This does not imply that each worker has 2 A10 GPUs and 2 A100 GPUs.","text":"<p>rayStartParams:   resources: '\"{\\\"accelerator_type_cpu\\\": 48, \\\"accelerator_type_a10\\\": 2, \\\"accelerator_type_a100_80g\\\": 2}\"' ```</p>"},{"location":"kuberay/deploy-on-eks/#step-2-deploy-an-llm-model-with-rayllm","title":"Step 2: Deploy an LLM model with RayLLM","text":"<pre><code># Step 7.1: Log in to the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- bash\n\n# Step 7.2: Llama 2 related models require HUGGING_FACE_HUB_TOKEN to be set.\n# If you don't have one, you can skip this step and deploy other models in Step 7.3.\nexport HUGGING_FACE_HUB_TOKEN=${YOUR_HUGGING_FACE_HUB_TOKEN}\n\n# Step 7.3: Deploy an LLM model. You can deploy Falcon-7B if you don't have a Hugging Face Hub token.\n# (1) Llama 2 7B\nserve run serve_configs/meta-llama--Llama-2-7b-chat-hf.yaml\n\n# Step 7.3: Check the Serve application status\nserve status\n\n# [Example output]\n# proxies:\n#   e4dc8d29f19e3900c0b93dabb76ce9bcc6f42e36bdf5484ca57ec774: HEALTHY\n#   4f4edf80bf644846175eec0a4daabb3f3775e64738720b6b2ae5c139: HEALTHY\n# applications:\n#   router:\n#     status: RUNNING\n#     message: ''\n#     last_deployed_time_s: 1694808658.0861287\n#     deployments:\n#       Router:\n#         status: HEALTHY\n#         replica_states:\n#           RUNNING: 2\n#         message: ''\n#   meta-llama--Llama-2-7b-chat-hf:\n#     status: RUNNING\n#     message: ''\n#     last_deployed_time_s: 1694808658.0861287\n#     deployments:\n#       meta-llama--Llama-2-7b-chat-hf:\n#         status: HEALTHY\n#         replica_states:\n#           RUNNING: 1\n#         message: ''\n\n# Step 7.4: Check the live Serve app's config\nserve config\n\n# [Example output]\n# name: router\n# route_prefix: /\n# import_path: rayllm.backend:router_application\n# args:\n#   models:\n#     meta-llama/Llama-2-7b-chat-hf: ./models/continuous_batching/meta-llama--Llama-2-7b-chat-hf.yaml\n\n# ---\n\n# name: meta-llama--Llama-2-7b-chat-hf\n# route_prefix: /meta-llama--Llama-2-7b-chat-hf\n# import_path: rayllm.backend:llm_application\n# args:\n#   model: ./models/continuous_batching/meta-llama--Llama-2-7b-chat-hf.yaml\n\n# Step 7.5: Send a query to `meta-llama/Llama-2-7b-chat-hf`.\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages?\"}\n    ],\n    \"temperature\": 0.7\n  }'\n\n# [Example output for `meta-llama/Llama-2-7b-chat-hf`]\n{\n  \"id\":\"meta-llama/Llama-2-7b-chat-hf-95239f0b-4601-4557-8a33-3977e9b6b779\",\n  \"object\":\"text_completion\",\"created\":1694814804,\"model\":\"meta-llama/Llama-2-7b-chat-hf\",\n  \"choices\":[\n    {\n      \"message\":\n      {\n        \"role\":\"assistant\",\n        \"content\":\"As a helpful assistant, I'm glad to provide you with the top 5 most popular programming languages based on various sources and metrics:\\n\\n1. Java: Java is a popular language used for developing enterprise-level applications, Android apps, and web applications. It's known for its platform independence, which allows Java developers to create applications that can run on any device supporting the Java Virtual Machine (JVM).\\n\\n2. Python: Python is a versatile language that's widely used in various industries, including web development, data science, artificial intelligence, and machine learning. Its simplicity, readability, and ease of use make it a favorite among developers.\\n\\n3. JavaScript: JavaScript is the language of the web and is used for creating interactive client-side functionality for web applications. It's also used in mobile app development, game development, and server-side programming.\\n\\n4. C++: C++ is a high-performance language used for developing operating systems, games, and other high-performance applications. It's known for its efficiency, speed, and flexibility, making it a popular choice among developers.\\n\\n5. PHP: PHP is a server-side scripting language used for web development, especially for building dynamic websites and web applications. It's known for its ease of use and is widely used in the web development community.\\n\\nThese are the top 5 most popular programming languages based on various sources, but it's worth noting that programming language popularity can vary depending on the source and the time frame considered.\"\n      },\n      \"index\":0,\n      \"finish_reason\":\"stop\"\n    }\n  ],\n  \"usage\":{\n    \"prompt_tokens\":39,\n    \"completion_tokens\":330,\n    \"total_tokens\":369\n  }\n}\n</code></pre>"},{"location":"kuberay/deploy-on-eks/#part-4-deploy-rayllm-on-a-rayservice-recommended-for-production","title":"Part 4: Deploy RayLLM on a RayService (recommended for production)","text":""},{"location":"kuberay/deploy-on-eks/#step-1-create-a-rayservice-with-rayllm","title":"Step 1: Create a RayService with RayLLM","text":"<pre><code># path: docs/kuberay\nkubectl apply -f ray-service.rayllm-eks.yaml\n</code></pre> <p>The <code>spec.rayClusterConfig</code> in <code>ray-service.rayllm-eks.yaml</code> is the same as the <code>spec</code> in <code>ray-cluster.rayllm-eks.yaml</code>. The only difference lies in the <code>serve</code> port, which is required for both the Ray head and Ray worker Pods in the case of RayService. Hence, you can refer to Part 3 for more details about how to configure the RayCluster.</p> <p>Note: Both <code>amazon/LightGPT</code> and <code>OpenAssistant/falcon-7b-sft-top1-696</code> should take about 5 minutes to become ready. If this process takes longer, follow the instructions in the RayService troubleshooting guide to check the Ray dashboard.</p> <pre><code>serveConfigV2: |\n    applications:\n    - name: router\n      import_path: rayllm.backend:router_application\n      route_prefix: /\n      args:\n        models:\n          - ./models/continuous_batching/amazon--LightGPT.yaml\n          - ./models/continuous_batching/OpenAssistant--falcon-7b-sft-top1-696.yaml\n</code></pre> <p>In the YAML file, we use the <code>serveConfigV2</code> field to configure two LLM Serve applications, one for LightGPT and one for Falcon-7B. It's important to note that the <code>model</code> argument refers to the path of the LLM model's YAML file, located in the Ray head Pod.</p>"},{"location":"kuberay/deploy-on-eks/#step-2-send-a-query-to-both-amazonlightgpt-and-openassistantfalcon-7b-sft-top1-696","title":"Step 2: Send a query to both <code>amazon/LightGPT</code> and <code>OpenAssistant/falcon-7b-sft-top1-696</code>.","text":"<pre><code># Step 2.1: Port forward the Kubernetes Serve service.\n# Note that the service will be created only when all Serve applications are ready.\nkubectl get svc # Check if `aviary-serve-svc` is created.\nkubectl port-forward service/aviary-serve-svc 8000:8000\n\n# Step 2.2: Check that the models have started running using `serve status`\nserve status\n\n# [Example output]\n# Connecting to Aviary backend at:  http://localhost:8000/v1\n# OpenAssistant/falcon-7b-sft-top1-696\n# amazon/LightGPT\n\n# Step 2.3: Send a query to `amazon/LightGPT`.\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"amazon/LightGPT\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages?\"}\n    ],\n    \"temperature\": 0.7\n  }'\n\n# [Example output]\n# amazon/LightGPT:\n# 1. Java\n# 2. C++\n# 3. JavaScript\n# 4. Python\n# 5. SQL\n\n# Step 2.4: Send a query to `OpenAssistant/falcon-7b-sft-top1-696`.\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"OpenAssistant/falcon-7b-sft-top1-696\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages?\"}\n    ],\n    \"temperature\": 0.7\n  }'\n\n# [Example output for `OpenAssistant/falcon-7b-sft-top1-696`]\n# Connecting to Aviary backend at:  http://localhost:8000/v1\n# OpenAssistant/falcon-7b-sft-top1-696:\n# The top five most popular programming languages globally, according to TIOBE, are Java, Python, C, C++, and JavaScript. However, popularity can vary by region, industry, and\n# other factors. Additionally, the definition of a programming language can vary, leading to different rankings depending on the methodology used. Some rankings may include or\n# exclude specific scripting languages or high-level language variants, for example.\n\n# Here are some additional rankings of the most popular programming languages:\n# * **Top 10 programming languages in 2023**: Python, JavaScript, C#, Java, PHP, TypeScript, Swift, Golang, Ruby, and Kotlin.\n# [Source](https://www.toptal.com/software/programming-languages/2023-best-programming-languages/)\n# * **Top 10 programming languages in 2022**: Python, JavaScript, Java, C++, C#, PHP, Swift, Kotlin, R, and TypeScript.\n# [Source](https://www.toptal.com/software/programming-languages/2022-best-programming-languages/)\n# * **Top 10 programming languages in 2021**: Python, JavaScript, Java, C++, C#, PHP, Swift, Go, Kotlin, and TypeScript.\n# .....\n# These rankings can change frequently, so it's important to keep up to date with the latest trends.\n</code></pre> <p>Check out the RayLLM README to learn more ways to query models, such as with the Python <code>requests</code> library or the OpenAI package. Use these techniques to stream responses from the models.</p>"},{"location":"kuberay/deploy-on-eks/#part-5-clean-up-resources","title":"Part 5: Clean up resources","text":"<p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p> <pre><code># path: docs/kuberay\n# Case 1: RayLLM was deployed on a RayCluster\nkubectl delete -f ray-cluster.rayllm-eks.yaml\n# Case 2: RayLLM was deployed as a RayService\nkubectl delete -f ray-service.rayllm-eks.yaml\n\n# Uninstall the KubeRay operator chart\nhelm uninstall kuberay-operator\n\n# Delete the Amazon EKS cluster via AWS Web UI\n</code></pre>"},{"location":"kuberay/deploy-on-gke/","title":"Deploy RayLLM on Google Kubernetes Engine (GKE) using KubeRay","text":"<p>In this tutorial, we will:</p> <ol> <li>Set up a Kubernetes cluster on GKE.</li> <li>Deploy the KubeRay operator and a Ray cluster on GKE.</li> <li> <p>Run an LLM model with Ray Serve.</p> </li> <li> <p>Note that this document will be extended to include Ray autoscaling and the deployment of multiple models in the near future.</p> </li> </ol>"},{"location":"kuberay/deploy-on-gke/#step-1-create-a-kubernetes-cluster-on-gke","title":"Step 1: Create a Kubernetes cluster on GKE","text":"<p>Run this command and all following commands on your local machine or on the Google Cloud Shell. If running from your local machine, you will need to install the Google Cloud SDK.</p> <pre><code>gcloud container clusters create rayllm-gpu-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-standard-8\n</code></pre> <p>This command creates a Kubernetes cluster named <code>rayllm-gpu-cluster</code> with 1 node in the <code>us-west1-b</code> zone. In this example, we use the <code>e2-standard-8</code> machine type, which has 8 vCPUs and 32 GB RAM. The cluster has autoscaling enabled, so the number of nodes can increase or decrease based on the workload.</p> <p>You can also create a cluster from the Google Cloud Console.</p>"},{"location":"kuberay/deploy-on-gke/#step-2-create-a-gpu-node-pool","title":"Step 2: Create a GPU node pool","text":"<p>Run the following command to create a GPU node pool for Ray GPU workers. (You can also create it from the Google Cloud Console; see the GKE documentation for more details.)</p> <pre><code>gcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4-vws,count=4 \\\n  --zone us-west1-b \\\n  --cluster rayllm-gpu-cluster \\\n  --num-nodes 1 \\\n  --min-nodes 0 \\\n  --max-nodes 1 \\\n  --enable-autoscaling \\\n  --machine-type g2-standard-48 \\\n  --node-taints=ray.io/node-type=worker:NoSchedule \n</code></pre> <p>The <code>--accelerator</code> flag specifies the type and number of GPUs for each node in the node pool. In this example, we use the NVIDIA L4 GPU. The machine type <code>g2-standard-48</code> has 4 GPUs, 48 vCPUs and 192 GB RAM.</p> <p>Because this tutorial is for deploying 1 LLM, the maximum size of this GPU node pool is 1. If you want to deploy multiple LLMs in this cluster, you may need to increase the value of the max size.</p> <p>The taint <code>ray.io/node-type=worker:NoSchedule</code> prevents CPU-only Pods such as the Kuberay operator, Ray head, and CoreDNS pods from being scheduled on this GPU node pool. This is because GPUs are expensive, so we want to use this node pool for Ray GPU workers only.</p> <p>Concretely, any Pod that does not have the following toleration will not be scheduled on this GPU node pool:</p> <pre><code>tolerations:\n- key: ray.io/node-type\n  operator: Equal\n  value: worker\n  effect: NoSchedule\n</code></pre> <p>This toleration has already been added to the RayCluster YAML manifest <code>ray-cluster.rayllm-gke.yaml</code> used in Step 6.</p> <p>For more on taints and tolerations, see the Kubernetes documentation.</p>"},{"location":"kuberay/deploy-on-gke/#step-3-configure-kubectl-to-connect-to-the-cluster","title":"Step 3: Configure <code>kubectl</code> to connect to the cluster","text":"<p>Run the following command to download credentials and configure the Kubernetes CLI to use them.</p> <pre><code>gcloud container clusters get-credentials rayllm-gpu-cluster --zone us-west1-b\n</code></pre> <p>For more details, see the GKE documentation.</p>"},{"location":"kuberay/deploy-on-gke/#step-4-install-nvidia-gpu-device-drivers","title":"Step 4: Install NVIDIA GPU device drivers","text":"<p>This step is required for GPU support on GKE. See the GKE documentation for more details.</p> <pre><code># Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n\n# Verify that your nodes have allocatable GPUs \nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ...                                 4\n# ...                                 &lt;none&gt;\n</code></pre>"},{"location":"kuberay/deploy-on-gke/#step-5-install-the-kuberay-operator","title":"Step 5: Install the KubeRay operator","text":"<pre><code># Install both CRDs and KubeRay operator v0.6.0.\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n\n# It should be scheduled on the CPU node. If it is not, something is wrong.\n</code></pre>"},{"location":"kuberay/deploy-on-gke/#step-6-create-a-raycluster-with-rayllm","title":"Step 6: Create a RayCluster with RayLLM","text":"<p>If you are running this tutorial on the Google Cloud Shell, please copy the file <code>docs/kuberay/ray-cluster.rayllm-gke.yaml</code> to the Google Cloud Shell. You may find it useful to use the Cloud Shell Editor to edit the file.</p> <p>Now you can create a RayCluster with RayLLM. RayLLM is included in the image <code>anyscale/ray-llm:latest</code>, which is specified in the RayCluster YAML manifest <code>ray-cluster.rayllm-gke.yaml</code>.</p> <pre><code># path: docs/kuberay\nkubectl apply -f ray-cluster.rayllm-gke.yaml\n</code></pre> <p>Note the following aspects of the YAML file:</p> <ul> <li> <p>The <code>tolerations</code> for workers match the taints we specified in Step 2. This ensures that the Ray GPU workers are scheduled on the GPU node pool.</p> <p>```yaml</p> </li> <li> <p>The field <code>rayStartParams.resources</code> has been configured to allow Ray to schedule Ray tasks and actors appropriately. The <code>mosaicml--mpt-7b-chat.yaml</code> file uses two custom resources, <code>accelerator_type_cpu</code> and <code>accelerator_type_a10</code>.  See the Ray documentation for more details on resources.</p> <p>```yaml</p> </li> </ul>"},{"location":"kuberay/deploy-on-gke/#please-add-the-following-taints-to-the-gpu-node","title":"Please add the following taints to the GPU node.","text":"<p>tolerations:     - key: \"ray.io/node-type\"     operator: \"Equal\"     value: \"worker\"     effect: \"NoSchedule\" ```</p>"},{"location":"kuberay/deploy-on-gke/#ray-head-the-ray-head-has-a-pod-resource-limit-of-2-cpus","title":"Ray head: The Ray head has a Pod resource limit of 2 CPUs.","text":"<p>rayStartParams:   resources: '\"{\\\"accelerator_type_cpu\\\": 2}\"'</p>"},{"location":"kuberay/deploy-on-gke/#ray-workers-the-ray-worker-has-a-pod-resource-limit-of-48-cpus-and-4-gpus","title":"Ray workers: The Ray worker has a Pod resource limit of 48 CPUs and 4 GPUs.","text":"<p>rayStartParams:     resources: '\"{\\\"accelerator_type_cpu\\\": 48, \\\"accelerator_type_a10\\\": 4}\"' ```</p>"},{"location":"kuberay/deploy-on-gke/#step-7-deploy-an-llm-model-with-rayllm","title":"Step 7: Deploy an LLM model with RayLLM","text":"<pre><code># Step 7.1: Log in to the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- bash\n\n# Step 7.2: Deploy the `meta-llama/Llama-2-7b-chat-hf` model\nserve run serve_configs/meta-llama--Llama-2-7b-chat-hf.yaml\n\n# Step 7.3: Check the Serve application status\nserve status\n\n# [Example output]\n# proxies:\n#   e4dc8d29f19e3900c0b93dabb76ce9bcc6f42e36bdf5484ca57ec774: HEALTHY\n#   4f4edf80bf644846175eec0a4daabb3f3775e64738720b6b2ae5c139: HEALTHY\n# applications:\n#   router:\n#     status: RUNNING\n#     message: ''\n#     last_deployed_time_s: 1694808658.0861287\n#     deployments:\n#       Router:\n#         status: HEALTHY\n#         replica_states:\n#           RUNNING: 2\n#         message: ''\n#   meta-llama--Llama-2-7b-chat-hf:\n#     status: RUNNING\n#     message: ''\n#     last_deployed_time_s: 1694808658.0861287\n#     deployments:\n#       meta-llama--Llama-2-7b-chat-hf:\n#         status: HEALTHY\n#         replica_states:\n#           RUNNING: 1\n#         message: ''\n\n# Step 7.4: Check the live Serve app's config\nserve config\n\n# [Example output]\n# name: router\n# route_prefix: /\n# import_path: rayllm.backend:router_application\n# args:\n#   models:\n#     meta-llama/Llama-2-7b-chat-hf: ./models/continuous_batching/meta-llama--Llama-2-7b-chat-hf.yaml\n\n# ---\n\n# name: meta-llama--Llama-2-7b-chat-hf\n# route_prefix: /meta-llama--Llama-2-7b-chat-hf\n# import_path: rayllm.backend:llm_application\n# args:\n#   model: ./models/continuous_batching/meta-llama--Llama-2-7b-chat-hf.yaml\n\n# Step 7.5: Send a query to `meta-llama/Llama-2-7b-chat-hf`.\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages?\"}\n    ],\n    \"temperature\": 0.7\n  }'\n\n# [Example output for `meta-llama/Llama-2-7b-chat-hf`]\n{\n  \"id\":\"meta-llama/Llama-2-7b-chat-hf-95239f0b-4601-4557-8a33-3977e9b6b779\",\n  \"object\":\"text_completion\",\"created\":1694814804,\"model\":\"meta-llama/Llama-2-7b-chat-hf\",\n  \"choices\":[\n    {\n      \"message\":\n      {\n        \"role\":\"assistant\",\n        \"content\":\"As a helpful assistant, I'm glad to provide you with the top 5 most popular programming languages based on various sources and metrics:\\n\\n1. Java: Java is a popular language used for developing enterprise-level applications, Android apps, and web applications. It's known for its platform independence, which allows Java developers to create applications that can run on any device supporting the Java Virtual Machine (JVM).\\n\\n2. Python: Python is a versatile language that's widely used in various industries, including web development, data science, artificial intelligence, and machine learning. Its simplicity, readability, and ease of use make it a favorite among developers.\\n\\n3. JavaScript: JavaScript is the language of the web and is used for creating interactive client-side functionality for web applications. It's also used in mobile app development, game development, and server-side programming.\\n\\n4. C++: C++ is a high-performance language used for developing operating systems, games, and other high-performance applications. It's known for its efficiency, speed, and flexibility, making it a popular choice among developers.\\n\\n5. PHP: PHP is a server-side scripting language used for web development, especially for building dynamic websites and web applications. It's known for its ease of use and is widely used in the web development community.\\n\\nThese are the top 5 most popular programming languages based on various sources, but it's worth noting that programming language popularity can vary depending on the source and the time frame considered.\"\n      },\n      \"index\":0,\n      \"finish_reason\":\"stop\"\n    }\n  ],\n  \"usage\":{\n    \"prompt_tokens\":39,\n    \"completion_tokens\":330,\n    \"total_tokens\":369\n  }\n}\n</code></pre>"},{"location":"kuberay/deploy-on-gke/#step-8-clean-up-resources","title":"Step 8: Clean up resources","text":"<p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p> <pre><code># Step 8.1: Delete the RayCluster\n# path: docs/kuberay\nkubectl delete -f ray-cluster.rayllm-gke.yaml\n\n# Step 8.2: Uninstall the KubeRay operator chart\nhelm uninstall kuberay-operator\n\n# Step 8.3: Delete the GKE cluster\ngcloud container clusters delete rayllm-gpu-cluster\n</code></pre> <p>See the GKE documentation for more details on deleting a GKE cluster.</p>"}]}